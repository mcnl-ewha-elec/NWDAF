# setup environment
import pandas as pd
import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve,auc
from sklearn.metrics import confusion_matrix,plot_confusion_matrix


### load data ###
df = pd.read_csv('malware_data.csv', sep=';')
df.columns=df.columns.str.replace("android.permission.", "", regex=False)

print('THE DATA SET CONTAINS:\n{} Apps\n{} Permission Indicators'.format(df.shape[0], df.shape[1]))

### Data cleansing ###
# Delete columns with all elements 0 or all elements 1
for c in df.columns:
    if len(df[c].value_counts()) < 2:
        df.drop(columns=c, inplace = True)

print ('-> {} remaining columns\n'.format(df.shape[1]))

### Train, Test split ###
# 80% of the data for training
# 20% of the data for testing model performance
# FEATURE x: permission
# TARGET y: malware(1) / benign(0)

print('Total Observations: ')
data = df.drop(columns='type').values
target = df['type'].values
x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)

print ('{} training observations'.format(x_train.shape[0]))
print ('{} testing observations'.format(x_test.shape[0]), end='\n\n')

print('Training observations : ')
print('{} Non-malicious observations \n{} malicious observations'.format(np.bincount(y_train)[0], np.bincount(y_train)[1]), end='\n\n')

print('Testing observations : ')
print('{} Non-malicious observations \n{} malicious observations\n'.format(np.bincount(y_test)[0], np.bincount(y_test)[1]))

### define classifier ###

adaboost = AdaBoostClassifier(random_state=45)
adaboost.fit(x_train, y_train)
y_pred_AB = adaboost.predict(x_test)



# summarize feature importance for adaboost
feature = np.array(df.columns[0:92])
ada_feature= pd.DataFrame({'feature': feature,
                           'importance(AB)': adaboost.feature_importances_})
ada_feature.sort_values(by=['importance(AB)'], axis=0, ascending=False, inplace=True)
print('Important feature (Permission) of Adaboost Algorithm')
print(ada_feature.head(7), end='\n\n')

### Classification Results ###
# Accuracy, Precision, Recall, f1 score, ROC curve(& AUC), Binary Confusion Matrix
# Adaboost
print('result from Adaboost :\n')
print("train set Accuracy_score :", round(adaboost.score(x_train, y_train), 3))
print("Test set Accuracy_score :", round(accuracy_score(y_test, y_pred_AB), 3), '\n')

print("Test set precision score (malicious) :", round(precision_score(y_test, y_pred_AB, pos_label=1), 3))
print("Test set precision score (non-malicious) :", round(precision_score(y_test, y_pred_AB, pos_label=0), 3), '\n')

print("Test set recall score (malicious) :", round(recall_score(y_test, y_pred_AB, pos_label=1), 3))
print("Test set recall score (non-malicious) :", round(recall_score(y_test, y_pred_AB, pos_label=0), 3), '\n')

print("Test set f1 score (malicious) :", round(f1_score(y_test, y_pred_AB, pos_label=1), 3))
print("Test set f1 score (non-malicious) :", round(f1_score(y_test, y_pred_AB, pos_label=0), 3), '\n')


# ROC curve
# 면적을 측정하여 TPR(true positive rate: 실제 양성, 판단 양성) FPR(false positive rate: 실제 음성, 판단 양성) 복합적으로 평가
# area 'AUC': 1에 가까울수록 성능이 좋고 0.5(직선)에 가까울수록 성능이 나쁘다.
adB_fprs, adB_tprs, adB_thresholds = roc_curve(y_test, y_pred_AB, pos_label=1)
plt.plot(adB_fprs, adB_tprs, label='AdaBoost')
plt.plot([0.0, 1.0], [0.0, 1.0], '--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve using default parameter')
plt.legend()
plt.grid()
plt.show()

print('AdaBoost AUC={}'.format(round((auc(adB_fprs,adB_tprs)),3)))


# Binary confusion matrix

matrix=confusion_matrix(y_true=y_test,y_pred=y_pred_AB)
fig, ax = plt.subplots(figsize=(7.5, 7.5))
ax.matshow(matrix, cmap=plt.cm.Blues,alpha=0.3)
for i in range(matrix.shape[0]):
    for j in range(matrix.shape[1]):
        ax.text(x=j, y=i, s=matrix[i, j], va='center', ha='center', size='xx-large')
plt.xlabel('Predictions (Data Total= {})'.format(x_test.shape[0]), fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Adaboost Confusion Matrix', fontsize=18)
plt.show()
